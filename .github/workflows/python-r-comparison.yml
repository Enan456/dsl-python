name: Python-R Comparison Tests

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    # Run nightly to catch regressions
    - cron: '0 0 * * *'

jobs:
  comparison-test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ["3.9", "3.10", "3.11"]

    steps:
    - uses: actions/checkout@v3

    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}

    - name: Set up R
      uses: r-lib/actions/setup-r@v2
      with:
        r-version: '4.3'

    - name: Install R dependencies
      run: |
        R -e "install.packages(c('DSL'), repos='http://cran.rstudio.com/')"

    - name: Cache pip packages
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-

    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -e .

    - name: Run Python-R comparison tests
      id: comparison_tests
      run: |
        pytest tests/test_python_r_comparison.py -v \
          --html=comparison_report.html \
          --self-contained-html \
          --junitxml=test-results.xml
      continue-on-error: true

    - name: Generate detailed comparison report with metrics
      if: always()
      run: |
        python scripts/run_comparison.py \
          --report-dir reports \
          --save-metrics metrics.json

    - name: Extract and save metrics
      if: always()
      run: |
        python scripts/extract_metrics.py \
          --input reports/*/report.html \
          --output metrics_summary.json \
          --python-version ${{ matrix.python-version }}

    - name: Upload comparison report
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: comparison-report-py${{ matrix.python-version }}
        path: |
          comparison_report.html
          test-results.xml

    - name: Upload detailed report
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: detailed-report-py${{ matrix.python-version }}
        path: reports/

    - name: Upload metrics
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: metrics-py${{ matrix.python-version }}
        path: |
          metrics.json
          metrics_summary.json

    - name: Check test status and alert
      if: steps.comparison_tests.outcome == 'failure'
      run: |
        echo "::warning::Python-R comparison tests failed on Python ${{ matrix.python-version }}"
        echo "Check artifacts for detailed reports"

  comparison-summary:
    runs-on: ubuntu-latest
    needs: comparison-test
    if: always()

    steps:
    - uses: actions/checkout@v3

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Install dependencies
      run: |
        pip install matplotlib pandas numpy

    - name: Download all metrics
      uses: actions/download-artifact@v3
      with:
        path: all-metrics/

    - name: Aggregate metrics and generate dashboard
      run: |
        python scripts/aggregate_metrics.py \
          --input all-metrics/ \
          --output dashboard/ \
          --historical-data .metrics-history/

    - name: Upload dashboard
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: comparison-dashboard
        path: dashboard/

    - name: Commit metrics history
      if: github.event_name != 'pull_request'
      run: |
        git config --local user.email "github-actions[bot]@users.noreply.github.com"
        git config --local user.name "github-actions[bot]"
        mkdir -p .metrics-history
        cp dashboard/metrics_*.json .metrics-history/
        git add .metrics-history/
        git diff --quiet && git diff --staged --quiet || git commit -m "Update metrics history [skip ci]"
        git push
      continue-on-error: true

    - name: Generate status badge
      run: |
        python scripts/generate_badge.py \
          --metrics dashboard/metrics_summary.json \
          --output dashboard/badge.svg

    - name: Summary Comment
      run: |
        echo "### Python-R Comparison Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        python scripts/format_summary.py --metrics all-metrics/ >> $GITHUB_STEP_SUMMARY
